{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col, expr, to_date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "CHAIN= \"bsc\"\n",
    "DOMAIN = \"blockchain\"\n",
    "ROLE = \"ingestion\"\n",
    "DATA_TYPE = \"transactions\"\n",
    "NETWORK_TYPE = \"mainnet\"\n",
    "DATA_LAYER = \"bronze\"\n",
    "\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "\n",
    "# Kafka Topics\n",
    "TXS_TOPIC = f\"{DOMAIN}.{CHAIN}.{ROLE}.{DATA_TYPE}.raw\"\n",
    "\n",
    "TABLE_NAME = f\"{DATA_LAYER}.{CHAIN}_{NETWORK_TYPE}_{DATA_TYPE}\"\n",
    "CHECKPOINT_PATH = f\"s3a://datalake/_checkpoints/{TABLE_NAME}\"\n",
    "SUBJECT = f\"{TXS_TOPIC}-value\"\n",
    "\n",
    "avro_schema = requests.get(\n",
    "    f\"{SCHEMA_REGISTRY_URL}/subjects/{SUBJECT}/versions/latest\"\n",
    ").json()[\"schema\"]\n",
    "\n",
    "spark.conf.set(\"spark.sql.iceberg.write.distribution-mode\", \"hash\")\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- block_height: long (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- raw: string (nullable = true)\n",
      " |-- kafka_topic: string (nullable = true)\n",
      " |-- kafka_partition: integer (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- kafka_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", TXS_TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")   # latest (from latest offset when checkpoing file is not exist) \n",
    "    .option(\"maxOffsetsPerTrigger\", 500_000)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# .trigger(once=True) “把当前 Kafka 能读到的全部数据跑完，然后退出” , 如果不设置，streaming会永远进行\n",
    "# 它仍然会 从 earliest → latest 但 不会等待未来新数据\n",
    "\n",
    "df_stripped = df.withColumn(\n",
    "    \"value_no_header\",\n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ")\n",
    "\n",
    "df_parsed = (\n",
    "    df_stripped\n",
    "    .select(\n",
    "        # ===== Avro payload =====\n",
    "        from_avro(\n",
    "            col(\"value_no_header\"),\n",
    "            avro_schema,\n",
    "            {\"mode\": \"PERMISSIVE\"}\n",
    "        ).alias(\"r\"),\n",
    "\n",
    "        # ===== Kafka metadata =====\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\n",
    "        \"r.*\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# convert string to timestamp\n",
    "df_parsed_ts = (\n",
    "    df_parsed\n",
    "    .withColumn(\"kafka_date\", to_date(col(\"kafka_timestamp\")))\n",
    ")\n",
    "\n",
    "df_ordered = df_parsed_ts.selectExpr(\n",
    "    \"block_height\",\n",
    "    \"job_name\",\n",
    "    \"run_id\",\n",
    "    \"raw\",\n",
    "    \"kafka_topic\",\n",
    "    \"kafka_partition\",\n",
    "    \"kafka_offset\",\n",
    "    \"kafka_timestamp\",\n",
    "    \"kafka_date\"\n",
    ")\n",
    "\n",
    "df_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/03 14:03:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_realtime_query = (\n",
    "    df_ordered\n",
    "    .writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"60 seconds\") # Micro-batch 触发间隔; 极限 ≈ 500ms–1s\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .start(TABLE_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a939cb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_realtime_query.awaitTermination()\n",
    "start_realtime_query.isActive\n",
    "# start_realtime_query.status\n",
    "# start_realtime_query.lastProgress['sources'][0]\n",
    "# start_realtime_query.stop() # stop the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21776946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"\"\"select count(1) from bronze.bsc_mainnet_transactions\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb1024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                               |summary                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2026-02-03 13:54:15.478|7651103054733784388|7650246549845223993|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-7651103054733784388-1-e543ecbc-1d78-4740-b017-170dd14d3449.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 657, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 1151, added-files-size -> 336007, changed-partition-count -> 1, total-records -> 603042, total-files-size -> 164821048, total-data-files -> 653, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2026-02-03 13:54:10.719|7650246549845223993|2282939311818909513|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-7650246549845223993-1-b2fa7ab9-5ef0-4429-979a-e4ae430a1454.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 656, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 900, added-files-size -> 230351, changed-partition-count -> 1, total-records -> 601891, total-files-size -> 164485041, total-data-files -> 652, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2026-02-03 13:54:05.638|2282939311818909513|2006301568109980867|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-2282939311818909513-1-4308d8d5-6d1f-44c9-8711-fba7771a9387.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 655, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 2221, added-files-size -> 622693, changed-partition-count -> 1, total-records -> 600991, total-files-size -> 164254690, total-data-files -> 651, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2026-02-03 13:54:01.252|2006301568109980867|8227276163192433752|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-2006301568109980867-1-0390dd16-29c5-416d-a95b-1cdb46248e35.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 654, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 307, added-files-size -> 110960, changed-partition-count -> 1, total-records -> 598770, total-files-size -> 163631997, total-data-files -> 650, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2026-02-03 13:53:55.842|8227276163192433752|8272768668403974080|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-8227276163192433752-1-e22f8a53-8c3a-4a28-8205-82f569ed986c.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 653, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 296, added-files-size -> 78346, changed-partition-count -> 1, total-records -> 598463, total-files-size -> 163521037, total-data-files -> 649, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}  |\n",
      "|2026-02-03 13:53:50.816|8272768668403974080|4945439738518418864|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-8272768668403974080-1-cdcec463-d265-4691-9e80-20d83dfecdbc.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 652, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 1360, added-files-size -> 381207, changed-partition-count -> 1, total-records -> 598167, total-files-size -> 163442691, total-data-files -> 648, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2026-02-03 13:53:45.78 |4945439738518418864|8067398927746421287|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-4945439738518418864-1-fd78d4d9-c13f-4d46-a2dc-5219c8ca3559.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 651, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 830, added-files-size -> 228965, changed-partition-count -> 1, total-records -> 596807, total-files-size -> 163061484, total-data-files -> 647, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2026-02-03 13:53:40.748|8067398927746421287|5454280398849172015|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-8067398927746421287-1-0c71ad17-4b89-4e14-aaf6-119a2ad5d364.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 650, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 543, added-files-size -> 147266, changed-partition-count -> 1, total-records -> 595977, total-files-size -> 162832519, total-data-files -> 646, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0} |\n",
      "|2026-02-03 13:53:35.935|5454280398849172015|8911167307083989258|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-5454280398849172015-1-e199be06-ba98-4b42-8855-def2276bfe0d.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 649, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 1984, added-files-size -> 526581, changed-partition-count -> 1, total-records -> 595434, total-files-size -> 162685253, total-data-files -> 645, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "|2026-02-03 13:53:25.74 |8911167307083989258|2834577994278459479|append   |s3a://datalake/bronze/bsc_mainnet_transactions/metadata/snap-8911167307083989258-1-f97cbda6-7170-454c-865c-8ac35526d659.avro|{spark.app.id -> local-1770122875575, spark.sql.streaming.epochId -> 648, spark.sql.streaming.queryId -> 3874e286-5acf-46a7-9a93-56b7abeb6861, added-data-files -> 1, added-records -> 1248, added-files-size -> 335168, changed-partition-count -> 1, total-records -> 593450, total-files-size -> 162158672, total-data-files -> 644, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0}|\n",
      "+-----------------------+-------------------+-------------------+---------+----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM bronze.bsc_mainnet_transactions.snapshots order by committed_at desc limit 10\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
