{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col, expr, to_timestamp, to_date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "CHAIN= \"bsc\"\n",
    "DOMAIN = \"blockchain\"\n",
    "ROLE = \"ingestion\"\n",
    "DATA_TYPE = \"transactions\"\n",
    "NETWORK_TYPE = \"mainnet\"\n",
    "DATA_LAYER = \"bronze\"\n",
    "\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "\n",
    "# Kafka Topics\n",
    "TXS_TOPIC = f\"{DOMAIN}.{CHAIN}.{ROLE}.{DATA_TYPE}.raw\"\n",
    "\n",
    "TABLE_NAME = f\"{DATA_LAYER}.{CHAIN}_{NETWORK_TYPE}_{DATA_TYPE}\"\n",
    "CHECKPOINT_PATH = f\"s3a://datalake/_checkpoints/{TABLE_NAME}\"\n",
    "SUBJECT = f\"{TXS_TOPIC}-value\"\n",
    "\n",
    "avro_schema = requests.get(\n",
    "    f\"{SCHEMA_REGISTRY_URL}/subjects/{SUBJECT}/versions/latest\"\n",
    ").json()[\"schema\"]\n",
    "\n",
    "\n",
    "# Iceberg 写入优化, 避免小文件\n",
    "spark.conf.set(\"spark.sql.iceberg.write.target-file-size-bytes\", 512 * 1024 * 1024)\n",
    "spark.conf.set(\"spark.sql.iceberg.write.fanout-enabled\", \"true\") # 并行写入\n",
    "spark.conf.set(\"spark.sql.iceberg.write.distribution-mode\", \"range\") # range：更容易形成大连续文件\n",
    "\n",
    "# S3A / MinIO优化\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.multipart.size\", \"134217728\")  # 128MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- block_height: long (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- raw: string (nullable = true)\n",
      " |-- kafka_topic: string (nullable = true)\n",
      " |-- kafka_partition: integer (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      " |-- kafka_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", TXS_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")   # earliest (from first offset) 只在第一次启动时生效一次。\n",
    "    .option(\"maxOffsetsPerTrigger\", 1_000_000)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# .trigger(once=True) “把当前 Kafka 能读到的全部数据跑完，然后退出” \n",
    "# 它仍然会 从 earliest → latest 但 不会等待未来新数据\n",
    "\n",
    "df_stripped = df.withColumn(\n",
    "    \"value_no_header\",\n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ")\n",
    "\n",
    "df_parsed = (\n",
    "    df_stripped\n",
    "    .select(\n",
    "        # ===== Avro payload =====\n",
    "        from_avro(\n",
    "            col(\"value_no_header\"),\n",
    "            avro_schema,\n",
    "            {\"mode\": \"PERMISSIVE\"}\n",
    "        ).alias(\"r\"),\n",
    "\n",
    "        # ===== Kafka metadata =====\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\n",
    "        \"r.*\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# convert string to timestamp\n",
    "df_parsed_ts = (\n",
    "    df_parsed\n",
    "    .withColumn(\"kafka_date\", to_date(col(\"kafka_timestamp\")))\n",
    ")\n",
    "\n",
    "df_ordered = df_parsed_ts.selectExpr(\n",
    "    \"block_height\",\n",
    "    \"job_name\",\n",
    "    \"run_id\",\n",
    "    \"raw\",\n",
    "    \"kafka_topic\",\n",
    "    \"kafka_partition\",\n",
    "    \"kafka_offset\",\n",
    "    \"kafka_timestamp\",\n",
    "    \"kafka_date\"\n",
    ")\n",
    "\n",
    "df_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d0def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/03 12:35:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "start_backfill_query = (\n",
    "    df_ordered\n",
    "    .writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .trigger(availableNow=True) # 把“启动那一刻已经存在的数据”读完，然后退出。启动之后新进 Kafka 的数据，不会被保证处理完。\n",
    "    .start(TABLE_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c110198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_backfill_query.stop()\n",
    "start_backfill_query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ae8b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|225840  |\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select count(1) from bronze.bsc_mainnet_transactions\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8f1c7",
   "metadata": {},
   "source": [
    "| 字段             | 含义                           |\n",
    "| -------------- | ---------------------------- |\n",
    "| `startOffset`  | 本批次开始消费的位置                   |\n",
    "| `endOffset`    | **本批次消费到的“最后一个 offset + 1”** |\n",
    "| `latestOffset` | trigger 执行时 Kafka 的最新 offset |\n",
    "\n",
    "前提：\n",
    "1. Job1 必须完全结束\n",
    "2. Job1 和 Job2 使用 同一个 Kafka 集群 & topic\n",
    "3. Kafka 没有开启「非默认 offset reset 逻辑」\n",
    "    - retention 过小\n",
    "    - offset 被删除\n",
    "    - topic 被 truncate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9c3b9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'KafkaV2[Subscribe[blockchain.bsc.ingestion.transactions.raw]]',\n",
       "  'startOffset': {'blockchain.bsc.ingestion.transactions.raw': {'8': 832655,\n",
       "    '11': 833268,\n",
       "    '2': 834362,\n",
       "    '5': 834250,\n",
       "    '4': 833941,\n",
       "    '7': 833272,\n",
       "    '10': 832960,\n",
       "    '1': 833510,\n",
       "    '9': 832141,\n",
       "    '3': 833537,\n",
       "    '6': 832632,\n",
       "    '0': 833409}},\n",
       "  'endOffset': {'blockchain.bsc.ingestion.transactions.raw': {'8': 881081,\n",
       "    '11': 881729,\n",
       "    '2': 882887,\n",
       "    '5': 882768,\n",
       "    '4': 882442,\n",
       "    '7': 881734,\n",
       "    '10': 881403,\n",
       "    '1': 881986,\n",
       "    '9': 880537,\n",
       "    '3': 882014,\n",
       "    '6': 881057,\n",
       "    '0': 881878}},\n",
       "  'latestOffset': {'blockchain.bsc.ingestion.transactions.raw': {'8': 881081,\n",
       "    '11': 881729,\n",
       "    '2': 882887,\n",
       "    '5': 882768,\n",
       "    '4': 882442,\n",
       "    '7': 881734,\n",
       "    '10': 881403,\n",
       "    '1': 881986,\n",
       "    '9': 880537,\n",
       "    '3': 882014,\n",
       "    '6': 881057,\n",
       "    '0': 881878}},\n",
       "  'numInputRows': 437136,\n",
       "  'inputRowsPerSecond': 12193.812937599376,\n",
       "  'processedRowsPerSecond': 27894.582349562887,\n",
       "  'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "   'maxOffsetsBehindLatest': '0',\n",
       "   'minOffsetsBehindLatest': '0'}}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_backfill_query.lastProgress[\"sources\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
