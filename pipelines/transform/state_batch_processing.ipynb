{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col, expr, to_timestamp, to_date, current_timestamp, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "\n",
    "# Kafka Topics (compact topic for state)\n",
    "STATE_TOPIC = \"blockchain.ingestion._state\"\n",
    "TABLE_NAME = \"bronze.kafka_ingestion_state\"\n",
    "SUBJECT = f\"{STATE_TOPIC}-value\"\n",
    "\n",
    "avro_schema = requests.get(\n",
    "    f\"{SCHEMA_REGISTRY_URL}/subjects/{SUBJECT}/versions/latest\"\n",
    ").json()[\"schema\"]\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.iceberg.write.distribution-mode\", \"hash\")\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", STATE_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"endingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "df_stripped = df.withColumn(\n",
    "    \"value_no_header\",\n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ")\n",
    "\n",
    "df_parsed = (\n",
    "    df_stripped\n",
    "    .select(\n",
    "        # ===== Avro payload =====\n",
    "        from_avro(\n",
    "            col(\"value_no_header\"),\n",
    "            avro_schema,\n",
    "            {\"mode\": \"PERMISSIVE\"}\n",
    "        ).alias(\"r\"),\n",
    "        \n",
    "        # ===== Kafka key =====\n",
    "        col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "\n",
    "        # ===== Kafka metadata =====\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\n",
    "        \"r.*\",\n",
    "        \"kafka_key\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# convert string to timestamp\n",
    "df_parsed_ts = (\n",
    "    df_parsed\n",
    "    .withColumn(\"kafka_date\", to_date(col(\"kafka_timestamp\")))\n",
    ")\n",
    "\n",
    "df_ordered = (\n",
    "    df_parsed_ts\n",
    "    .select(\n",
    "        # ===== state core =====\n",
    "        col(\"checkpoint\").alias(\"checkpoint_block\"),\n",
    "\n",
    "        col(\"producer.pod_name\").alias(\"producer_pod_name\"),\n",
    "        col(\"producer.pod_uid\").alias(\"producer_pod_uid\"),\n",
    "\n",
    "        col(\"run.run_id\").alias(\"run_id\"),\n",
    "        col(\"run.mode\").alias(\"run_mode\"),\n",
    "        col(\"run.start_block\").alias(\"run_start_block\"),\n",
    "        to_timestamp(col(\"run.started_at\")).alias(\"run_started_at\"),\n",
    "        \n",
    "        # ===== Kafka metadata =====\n",
    "        \"kafka_key\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\",\n",
    "        \"kafka_date\"\n",
    "    )\n",
    "    .withColumn(\"state_updated_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "\n",
    "window_latest = Window.partitionBy(\"kafka_key\").orderBy(\n",
    "    col(\"kafka_offset\").desc()\n",
    ")\n",
    "\n",
    "df_latest = (\n",
    "    df_ordered\n",
    "    .withColumn(\"rn\", row_number().over(window_latest))\n",
    "    .filter(col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "df_latest.createOrReplaceTempView(\"state_updates\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO bronze.kafka_ingestion_state t\n",
    "USING state_updates s\n",
    "ON t.run_id = s.run_id\n",
    "\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  t.checkpoint_block       = s.checkpoint_block,\n",
    "  t.producer_pod_name      = s.producer_pod_name,\n",
    "  t.producer_pod_uid       = s.producer_pod_uid,\n",
    "  t.run_mode               = s.run_mode,\n",
    "  t.run_start_block        = s.run_start_block,\n",
    "  t.run_started_at         = s.run_started_at,\n",
    "  t.kafka_key              = s.kafka_key,\n",
    "  t.kafka_topic            = s.kafka_topic,\n",
    "  t.kafka_partition        = s.kafka_partition,\n",
    "  t.kafka_offset           = s.kafka_offset,\n",
    "  t.kafka_timestamp        = s.kafka_timestamp,\n",
    "  t.kafka_date             = s.kafka_date,\n",
    "  t.state_updated_at       = s.state_updated_at\n",
    "\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    with kafka_key_cte as (\n",
    "        select \n",
    "            split(kafka_key, ':') AS parts,\n",
    "            kafka_key,\n",
    "            run_start_block as run_start,\n",
    "            checkpoint_block as run_end,\n",
    "            producer_pod_name,\n",
    "            run_id,\n",
    "            run_mode,\n",
    "            kafka_timestamp\n",
    "        from bronze.kafka_ingestion_state\n",
    "    )\n",
    "    select \n",
    "        parts[0] AS chain,\n",
    "        parts[1] AS ingestion_entity,\n",
    "        parts[2] AS ingestion_type,\n",
    "        run_mode,\n",
    "        run_start,\n",
    "        run_end,\n",
    "        producer_pod_name,\n",
    "        run_id,\n",
    "        kafka_timestamp,\n",
    "        kafka_key\n",
    "        from (\n",
    "            select *, \n",
    "            row_number() over (partition by kafka_key order by run_end desc, kafka_timestamp desc, run_id desc) as rn\n",
    "        from kafka_key_cte\n",
    "        ) order by kafka_key, rn\n",
    "\"\"\").createOrReplaceTempView(\"v_latest_ingestion_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-3.0.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (11.2 MB)\n",
      "Using cached numpy-2.4.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
      "Installing collected packages: numpy, pandas\n",
      "Successfully installed numpy-2.4.2 pandas-3.0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "60e014a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chain</th>\n",
       "      <th>ingestion_entity</th>\n",
       "      <th>ingestion_type</th>\n",
       "      <th>run_mode</th>\n",
       "      <th>run_start</th>\n",
       "      <th>run_end</th>\n",
       "      <th>producer_pod_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>kafka_timestamp</th>\n",
       "      <th>kafka_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bsc</td>\n",
       "      <td>blocks</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79438840</td>\n",
       "      <td>79439545</td>\n",
       "      <td>bsc-blocks-ingestion-5db5bfc7d9-mzr7l</td>\n",
       "      <td>b61501db-9303-4ad9-9366-f8f14cbb3d47</td>\n",
       "      <td>2026-02-05 12:09:20.740</td>\n",
       "      <td>bsc:blocks:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bsc</td>\n",
       "      <td>blocks</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79430917</td>\n",
       "      <td>79438372</td>\n",
       "      <td>bsc-blocks-ingestion-5db5bfc7d9-mzr7l</td>\n",
       "      <td>8a2ee0ff-b49b-409e-8d2a-19810db3c51f</td>\n",
       "      <td>2026-02-05 12:00:53.506</td>\n",
       "      <td>bsc:blocks:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bsc</td>\n",
       "      <td>blocks</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79412001</td>\n",
       "      <td>79416288</td>\n",
       "      <td>bsc-blocks-ingestion-5db5bfc7d9-mzr7l</td>\n",
       "      <td>234574e2-7196-4ce8-9869-34b352bc5028</td>\n",
       "      <td>2026-02-05 09:14:48.655</td>\n",
       "      <td>bsc:blocks:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bsc</td>\n",
       "      <td>blocks</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79406931</td>\n",
       "      <td>79408433</td>\n",
       "      <td>bsc-blocks-ingestion-6669546d46-dcx6g</td>\n",
       "      <td>385ae973-c93c-4c39-8e82-21dd322a9a56</td>\n",
       "      <td>2026-02-05 08:15:52.709</td>\n",
       "      <td>bsc:blocks:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bsc</td>\n",
       "      <td>logs</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79439374</td>\n",
       "      <td>79439546</td>\n",
       "      <td>bsc-logs-ingestion-6c6df4f66-jdxf9</td>\n",
       "      <td>2132ae46-d5ee-4102-bcb7-a7dbf6e40b35</td>\n",
       "      <td>2026-02-05 12:09:19.691</td>\n",
       "      <td>bsc:logs:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bsc</td>\n",
       "      <td>logs</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79412002</td>\n",
       "      <td>79423691</td>\n",
       "      <td>bsc-logs-ingestion-69799c448-bqg4p</td>\n",
       "      <td>9c301d60-0703-4957-92b1-770bc8f055a3</td>\n",
       "      <td>2026-02-05 10:11:34.530</td>\n",
       "      <td>bsc:logs:realtime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bsc</td>\n",
       "      <td>logs</td>\n",
       "      <td>realtime</td>\n",
       "      <td>chain_head_resume</td>\n",
       "      <td>79405757</td>\n",
       "      <td>79405865</td>\n",
       "      <td>bsc-logs-ingestion-7f9b495d5f-fg499</td>\n",
       "      <td>795c9713-a01a-46ae-aea4-0dfa3870d52f</td>\n",
       "      <td>2026-02-05 07:56:39.632</td>\n",
       "      <td>bsc:logs:realtime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  chain ingestion_entity ingestion_type           run_mode  run_start  \\\n",
       "0   bsc           blocks       realtime  chain_head_resume   79438840   \n",
       "1   bsc           blocks       realtime  chain_head_resume   79430917   \n",
       "2   bsc           blocks       realtime  chain_head_resume   79412001   \n",
       "3   bsc           blocks       realtime  chain_head_resume   79406931   \n",
       "4   bsc             logs       realtime  chain_head_resume   79439374   \n",
       "5   bsc             logs       realtime  chain_head_resume   79412002   \n",
       "6   bsc             logs       realtime  chain_head_resume   79405757   \n",
       "\n",
       "    run_end                      producer_pod_name  \\\n",
       "0  79439545  bsc-blocks-ingestion-5db5bfc7d9-mzr7l   \n",
       "1  79438372  bsc-blocks-ingestion-5db5bfc7d9-mzr7l   \n",
       "2  79416288  bsc-blocks-ingestion-5db5bfc7d9-mzr7l   \n",
       "3  79408433  bsc-blocks-ingestion-6669546d46-dcx6g   \n",
       "4  79439546     bsc-logs-ingestion-6c6df4f66-jdxf9   \n",
       "5  79423691     bsc-logs-ingestion-69799c448-bqg4p   \n",
       "6  79405865    bsc-logs-ingestion-7f9b495d5f-fg499   \n",
       "\n",
       "                                 run_id         kafka_timestamp  \\\n",
       "0  b61501db-9303-4ad9-9366-f8f14cbb3d47 2026-02-05 12:09:20.740   \n",
       "1  8a2ee0ff-b49b-409e-8d2a-19810db3c51f 2026-02-05 12:00:53.506   \n",
       "2  234574e2-7196-4ce8-9869-34b352bc5028 2026-02-05 09:14:48.655   \n",
       "3  385ae973-c93c-4c39-8e82-21dd322a9a56 2026-02-05 08:15:52.709   \n",
       "4  2132ae46-d5ee-4102-bcb7-a7dbf6e40b35 2026-02-05 12:09:19.691   \n",
       "5  9c301d60-0703-4957-92b1-770bc8f055a3 2026-02-05 10:11:34.530   \n",
       "6  795c9713-a01a-46ae-aea4-0dfa3870d52f 2026-02-05 07:56:39.632   \n",
       "\n",
       "             kafka_key  \n",
       "0  bsc:blocks:realtime  \n",
       "1  bsc:blocks:realtime  \n",
       "2  bsc:blocks:realtime  \n",
       "3  bsc:blocks:realtime  \n",
       "4    bsc:logs:realtime  \n",
       "5    bsc:logs:realtime  \n",
       "6    bsc:logs:realtime  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from v_latest_ingestion_state\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "101f2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy execution\n",
    "# df_latest.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8ae8b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+--------------+-----------------+---------+--------+-------------------------------------+------------------------------------+-----------------------+-------------------+\n",
      "|chain|ingestion_entity|ingestion_type|run_mode         |run_start|run_end |producer_pod_name                    |run_id                              |kafka_timestamp        |kafka_key          |\n",
      "+-----+----------------+--------------+-----------------+---------+--------+-------------------------------------+------------------------------------+-----------------------+-------------------+\n",
      "|bsc  |blocks          |realtime      |chain_head_resume|79412001 |79415700|bsc-blocks-ingestion-5db5bfc7d9-mzr7l|234574e2-7196-4ce8-9869-34b352bc5028|2026-02-05 09:10:24.042|bsc:blocks:realtime|\n",
      "|bsc  |blocks          |realtime      |chain_head_resume|79406931 |79408433|bsc-blocks-ingestion-6669546d46-dcx6g|385ae973-c93c-4c39-8e82-21dd322a9a56|2026-02-05 08:15:52.709|bsc:blocks:realtime|\n",
      "|bsc  |logs            |realtime      |chain_head_resume|79412002 |79415697|bsc-logs-ingestion-69799c448-bqg4p   |9c301d60-0703-4957-92b1-770bc8f055a3|2026-02-05 09:10:24.727|bsc:logs:realtime  |\n",
      "|bsc  |logs            |realtime      |chain_head_resume|79405757 |79405865|bsc-logs-ingestion-7f9b495d5f-fg499  |795c9713-a01a-46ae-aea4-0dfa3870d52f|2026-02-05 07:56:39.632|bsc:logs:realtime  |\n",
      "+-----+----------------+--------------+-----------------+---------+--------+-------------------------------------+------------------------------------+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    with kafka_key_cte as (\n",
    "        select \n",
    "            split(kafka_key, ':') AS parts,\n",
    "            kafka_key,\n",
    "            run_start_block as run_start,\n",
    "            checkpoint_block as run_end,\n",
    "            producer_pod_name,\n",
    "            run_id,\n",
    "            run_mode,\n",
    "            kafka_timestamp\n",
    "        from bronze.kafka_ingestion_state\n",
    "    )\n",
    "    select \n",
    "        parts[0] AS chain,\n",
    "        parts[1] AS ingestion_entity,\n",
    "        parts[2] AS ingestion_type,\n",
    "        run_mode,\n",
    "        run_start,\n",
    "        run_end,\n",
    "        producer_pod_name,\n",
    "        run_id,\n",
    "        kafka_timestamp,\n",
    "        kafka_key\n",
    "        from (\n",
    "            select *, \n",
    "            row_number() over (partition by kafka_key order by run_end desc, kafka_timestamp desc, run_id desc) as rn\n",
    "        from kafka_key_cte\n",
    "        ) order by kafka_key, rn\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "95b4c465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          delete from bronze.kafka_ingestion_state where kafka_key like \"%.%\"\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c3714991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hadoop'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalog.spark_catalog.type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96298b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
