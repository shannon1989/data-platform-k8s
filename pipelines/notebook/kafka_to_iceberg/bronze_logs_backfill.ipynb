{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col, expr, to_timestamp, to_date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "BLOCKS_TOPIC = \"blockchain.logs.base\"\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "TABLE_NAME = \"bronze.base_mainnet_logs\"\n",
    "CHECKPOINT_PATH = f\"s3a://datalake/_checkpoints/{TABLE_NAME}\"\n",
    "SUBJECT = f\"{BLOCKS_TOPIC}-value\"\n",
    "\n",
    "avro_schema = requests.get(\n",
    "    f\"{SCHEMA_REGISTRY_URL}/subjects/{SUBJECT}/versions/latest\"\n",
    ").json()[\"schema\"]\n",
    "\n",
    "\n",
    "# Iceberg 写入优化, 避免小文件\n",
    "spark.conf.set(\"spark.sql.iceberg.write.target-file-size-bytes\", 512 * 1024 * 1024)\n",
    "spark.conf.set(\"spark.sql.iceberg.write.distribution-mode\", \"hash\")\n",
    "\n",
    "# S3A / MinIO优化\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload\", \"true\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.fast.upload.buffer\", \"disk\")\n",
    "spark.conf.set(\"spark.hadoop.fs.s3a.multipart.size\", \"134217728\")  # 128MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- block_height: long (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- inserted_date: date (nullable = true)\n",
      " |-- raw: string (nullable = true)\n",
      " |-- kafka_topic: string (nullable = true)\n",
      " |-- kafka_partition: integer (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", BLOCKS_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")   # earliest (from first offset) 只在第一次启动时生效一次。\n",
    "    .option(\"maxOffsetsPerTrigger\", 300_000)  \n",
    "    .load()\n",
    ")\n",
    "\n",
    "# .trigger(once=True) “把当前 Kafka 能读到的全部数据跑完，然后退出” \n",
    "# 它仍然会 从 earliest → latest 但 不会等待未来新数据\n",
    "\n",
    "df_stripped = df.withColumn(\n",
    "    \"value_no_header\",\n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ")\n",
    "\n",
    "df_parsed = (\n",
    "    df_stripped\n",
    "    .select(\n",
    "        # ===== Avro payload =====\n",
    "        from_avro(\n",
    "            col(\"value_no_header\"),\n",
    "            avro_schema,\n",
    "            {\"mode\": \"PERMISSIVE\"}\n",
    "        ).alias(\"r\"),\n",
    "\n",
    "        # ===== Kafka metadata =====\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\n",
    "        \"r.*\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# convert string to timestamp\n",
    "df_parsed_ts = (\n",
    "    df_parsed\n",
    "    .withColumn(\"inserted_at\", to_timestamp(col(\"inserted_at\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"))\n",
    "    .withColumn(\"inserted_date\", to_date(col(\"inserted_at\")))\n",
    ")\n",
    "\n",
    "df_ordered = df_parsed_ts.selectExpr(\n",
    "    \"block_height\",\n",
    "    \"job_name\",\n",
    "    \"run_id\",\n",
    "    \"inserted_at\",\n",
    "    \"inserted_date\",\n",
    "    \"raw\",\n",
    "    \"kafka_topic\",\n",
    "    \"kafka_partition\",\n",
    "    \"kafka_offset\",\n",
    "    \"kafka_timestamp\"\n",
    ")\n",
    "\n",
    "df_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1d0def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/16 12:37:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 400:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "state_backfill_query = (\n",
    "    df_ordered\n",
    "    .writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .trigger(availableNow=True) # 把“启动那一刻已经存在的数据”读完，然后退出。启动之后新进 Kafka 的数据，不会被保证处理完。\n",
    "    .start(TABLE_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c110198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_backfill_query.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ae8b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|max(block_height)|max(kafka_offset)|\n",
      "+-----------------+-----------------+\n",
      "|40876520         |66205076         |\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        max(block_height), \n",
    "        max(kafka_offset)\n",
    "    from bronze.base_mainnet_logs\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8f1c7",
   "metadata": {},
   "source": [
    "| 字段             | 含义                           |\n",
    "| -------------- | ---------------------------- |\n",
    "| `startOffset`  | 本批次开始消费的位置                   |\n",
    "| `endOffset`    | **本批次消费到的“最后一个 offset + 1”** |\n",
    "| `latestOffset` | trigger 执行时 Kafka 的最新 offset |\n",
    "\n",
    "前提：\n",
    "1. Job1 必须完全结束\n",
    "2. Job1 和 Job2 使用 同一个 Kafka 集群 & topic\n",
    "3. Kafka 没有开启「非默认 offset reset 逻辑」\n",
    "    - retention 过小\n",
    "    - offset 被删除\n",
    "    - topic 被 truncate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9c3b9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'KafkaV2[Subscribe[blockchain.logs.base]]',\n",
       " 'startOffset': {'blockchain.logs.base': {'0': 66000000}},\n",
       " 'endOffset': {'blockchain.logs.base': {'0': 66205077}},\n",
       " 'latestOffset': {'blockchain.logs.base': {'0': 66205077}},\n",
       " 'numInputRows': 204998,\n",
       " 'inputRowsPerSecond': 77709.62850644428,\n",
       " 'processedRowsPerSecond': 121300.59171597633,\n",
       " 'metrics': {'avgOffsetsBehindLatest': '0.0',\n",
       "  'maxOffsetsBehindLatest': '0',\n",
       "  'minOffsetsBehindLatest': '0'}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_backfill_query.lastProgress['sources'][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
