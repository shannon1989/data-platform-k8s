{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col, expr, to_timestamp, to_date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "BLOCKS_TOPIC = \"blockchain.logs.base\"\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "TABLE_NAME = \"bronze.base_mainnet_logs\"\n",
    "CHECKPOINT_PATH = f\"s3a://datalake/_checkpoints/{TABLE_NAME}\"\n",
    "SUBJECT = f\"{BLOCKS_TOPIC}-value\"\n",
    "\n",
    "avro_schema = requests.get(\n",
    "    f\"{SCHEMA_REGISTRY_URL}/subjects/{SUBJECT}/versions/latest\"\n",
    ").json()[\"schema\"]\n",
    "\n",
    "spark.conf.set(\"spark.sql.iceberg.write.distribution-mode\", \"hash\")\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 1_000_000)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- block_height: long (nullable = true)\n",
      " |-- job_name: string (nullable = true)\n",
      " |-- run_id: string (nullable = true)\n",
      " |-- inserted_at: timestamp (nullable = true)\n",
      " |-- inserted_date: date (nullable = true)\n",
      " |-- raw: string (nullable = true)\n",
      " |-- kafka_topic: string (nullable = true)\n",
      " |-- kafka_partition: integer (nullable = true)\n",
      " |-- kafka_offset: long (nullable = true)\n",
      " |-- kafka_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", BLOCKS_TOPIC)\n",
    "    .option(\"startingOffsets\", \"latest\")   # latest(from latest offset when checkpoing file is not exist) \n",
    "    .option(\"maxOffsetsPerTrigger\", 500_000)\n",
    "    .option(\"minPartitions\", 4)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# .trigger(once=True) “把当前 Kafka 能读到的全部数据跑完，然后退出” , 如果不设置，streaming会永远进行\n",
    "# 它仍然会 从 earliest → latest 但 不会等待未来新数据\n",
    "\n",
    "df_stripped = df.withColumn(\n",
    "    \"value_no_header\",\n",
    "    expr(\"substring(value, 6, length(value)-5)\")\n",
    ")\n",
    "\n",
    "df_parsed = (\n",
    "    df_stripped\n",
    "    .select(\n",
    "        # ===== Avro payload =====\n",
    "        from_avro(\n",
    "            col(\"value_no_header\"),\n",
    "            avro_schema,\n",
    "            {\"mode\": \"PERMISSIVE\"}\n",
    "        ).alias(\"r\"),\n",
    "\n",
    "        # ===== Kafka metadata =====\n",
    "        col(\"topic\").alias(\"kafka_topic\"),\n",
    "        col(\"partition\").alias(\"kafka_partition\"),\n",
    "        col(\"offset\").alias(\"kafka_offset\"),\n",
    "        col(\"timestamp\").alias(\"kafka_timestamp\")\n",
    "    )\n",
    "    .select(\n",
    "        \"r.*\",\n",
    "        \"kafka_topic\",\n",
    "        \"kafka_partition\",\n",
    "        \"kafka_offset\",\n",
    "        \"kafka_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# convert string to timestamp\n",
    "df_parsed_ts = (\n",
    "    df_parsed\n",
    "    .withColumn(\"inserted_at\", to_timestamp(col(\"inserted_at\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"))\n",
    "    .withColumn(\"inserted_date\", to_date(col(\"inserted_at\")))\n",
    ")\n",
    "\n",
    "df_ordered = df_parsed_ts.selectExpr(\n",
    "    \"block_height\",\n",
    "    \"job_name\",\n",
    "    \"run_id\",\n",
    "    \"inserted_at\",\n",
    "    \"inserted_date\",\n",
    "    \"raw\",\n",
    "    \"kafka_topic\",\n",
    "    \"kafka_partition\",\n",
    "    \"kafka_offset\",\n",
    "    \"kafka_timestamp\"\n",
    ")\n",
    "\n",
    "df_ordered.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 07:41:54 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/17 07:41:54 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '8' to '200'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_out = (\n",
    "    df_ordered\n",
    "    .coalesce(4)\n",
    ")\n",
    "\n",
    "state_realtime_query = (\n",
    "    df_out\n",
    "    .writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(processingTime=\"60 seconds\") # Micro-batch 触发间隔\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .start(TABLE_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a939cb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state_realtime_query.awaitTermination()\n",
    "# state_realtime_query.isActive\n",
    "# state_realtime_query.status\n",
    "# state_realtime_query.lastProgress['sources'][0]\n",
    "# state_realtime_query.stop() # stop the streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0361be72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+-----------+-------------+---+-----------+---------------+------------+---------------+\n",
      "|block_height|job_name|run_id|inserted_at|inserted_date|raw|kafka_topic|kafka_partition|kafka_offset|kafka_timestamp|\n",
      "+------------+--------+------+-----------+-------------+---+-----------+---------------+------------+---------------+\n",
      "+------------+--------+------+-----------+-------------+---+-----------+---------------+------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# data missing: \"batch_range_start\": 40885992, \"batch_range_end\": 40886001,\n",
    "spark.sql(\"\"\"\n",
    "\n",
    "select * from bronze.base_mainnet_logs where block_height between 40885992 and 40886001\n",
    "          \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3579b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|max(block_height)|max(kafka_offset)|\n",
      "+-----------------+-----------------+\n",
      "|40887679         |76951887         |\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select max(block_height), max(kafka_offset) from bronze.base_mainnet_logs\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
