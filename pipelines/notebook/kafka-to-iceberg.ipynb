{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f8277b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/11 08:51:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/11 08:51:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/11 08:51:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "26/01/11 08:51:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "import fastavro\n",
    "import os\n",
    "from io import BytesIO\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, StructType, StructField\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"redpanda.kafka.svc:9092\")\n",
    "STATE_TOPIC = os.getenv(\"STATE_TOPIC\", \"blockchain.ingestion-state.eth.mainnet\")\n",
    "BLOCKS_TOPIC = os.getenv(\"BLOCKS_TOPIC\", \"blockchain.blocks.eth.mainnet\")\n",
    "\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "\n",
    "# 定义 schema\n",
    "blocks_schema = StructType([\n",
    "    StructField(\"block_height\", LongType()),\n",
    "    StructField(\"job_name\", StringType()),\n",
    "    StructField(\"run_id\", StringType()),\n",
    "    StructField(\"inserted_at\", StringType()),\n",
    "    StructField(\"raw\", StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01361f4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m      7\u001b[0m decode_udf \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;28;01mlambda\u001b[39;00m x: decode_avro(x), blocks_schema)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 读取 Kafka\u001b[39;00m\n\u001b[1;32m     10\u001b[0m blocks_df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKAFKA_BROKER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBLOCKS_TOPIC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, decode_udf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# 用 fastavro 解码 bytes\n",
    "def decode_avro(avro_bytes):\n",
    "    bio = BytesIO(avro_bytes)\n",
    "    record = fastavro.reader(bio)\n",
    "    return list(record)[0]  # Kafka 每条消息通常是单条 record\n",
    "\n",
    "decode_udf = udf(lambda x: decode_avro(x), blocks_schema)\n",
    "\n",
    "# 读取 Kafka\n",
    "blocks_df = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)\n",
    "    .option(\"subscribe\", BLOCKS_TOPIC)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .withColumn(\"decoded\", decode_udf(\"value\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbabeb41",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'from_avro' from 'pyspark.sql.functions' (/opt/conda/lib/python3.11/site-packages/pyspark/sql/functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_avro, col\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, LongType, StringType\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'from_avro' from 'pyspark.sql.functions' (/opt/conda/lib/python3.11/site-packages/pyspark/sql/functions.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_avro, col\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "KAFKA_BROKER = os.getenv(\"KAFKA_BROKER\", \"redpanda.kafka.svc:9092\")\n",
    "STATE_TOPIC = os.getenv(\"STATE_TOPIC\", \"blockchain.ingestion-state.eth.mainnet\")\n",
    "BLOCKS_TOPIC = os.getenv(\"BLOCKS_TOPIC\", \"blockchain.blocks.eth.mainnet\")\n",
    "\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23108e6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'from_avro' from 'pyspark.sql.functions' (/opt/conda/lib/python3.11/site-packages/pyspark/sql/functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_avro, col\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, LongType, StringType\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'from_avro' from 'pyspark.sql.functions' (/opt/conda/lib/python3.11/site-packages/pyspark/sql/functions.py)"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# 2️⃣ 定义 Avro Schema\n",
    "# ===============================\n",
    "blocks_avro_schema = \"\"\"\n",
    "{\n",
    "  \"type\":\"record\",\n",
    "  \"name\":\"BlockEvent\",\n",
    "  \"namespace\":\"platform.ingestion.blocks\",\n",
    "  \"fields\":[\n",
    "    {\"name\":\"block_height\",\"type\":\"long\"},\n",
    "    {\"name\":\"job_name\",\"type\":\"string\"},\n",
    "    {\"name\":\"run_id\",\"type\":\"string\"},\n",
    "    {\"name\":\"inserted_at\",\"type\":\"string\"},\n",
    "    {\"name\":\"raw\",\"type\":\"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "state_avro_schema = \"\"\"\n",
    "{\n",
    "  \"type\":\"record\",\n",
    "  \"name\":\"IngestionState\",\n",
    "  \"namespace\":\"platform.ingestion.state\",\n",
    "  \"fields\":[\n",
    "    {\"name\":\"job_name\",\"type\":\"string\"},\n",
    "    {\"name\":\"run_id\",\"type\":\"string\"},\n",
    "    {\"name\":\"range\",\"type\":{\n",
    "        \"type\":\"record\",\n",
    "        \"name\":\"BlockRange\",\n",
    "        \"fields\":[\n",
    "            {\"name\":\"start\",\"type\":\"long\"},\n",
    "            {\"name\":\"end\",\"type\":\"long\"}\n",
    "        ]\n",
    "    }},\n",
    "    {\"name\":\"checkpoint\",\"type\":\"long\"},\n",
    "    {\"name\":\"status\",\"type\":{\"type\":\"enum\",\"name\":\"IngestionStatus\",\"symbols\":[\"running\",\"stopped\",\"completed\",\"failed\"]}},\n",
    "    {\"name\":\"inserted_at\",\"type\":\"string\"}\n",
    "  ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbbd43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 3️⃣ 从 Kafka 读取 Avro 数据\n",
    "# ===============================\n",
    "blocks_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap)\n",
    "    .option(\"subscribe\", \"blockchain.blocks.eth.mainnet\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS BINARY) as avro_bytes\")\n",
    "    .select(from_avro(col(\"avro_bytes\"), blocks_avro_schema, {\"schema.registry.url\": schema_registry}).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "state_df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap)\n",
    "    .option(\"subscribe\", \"blockchain.ingestion-state.eth.mainnet\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr(\"CAST(value AS BINARY) as avro_bytes\")\n",
    "    .select(from_avro(col(\"avro_bytes\"), state_avro_schema, {\"schema.registry.url\": schema_registry}).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 4️⃣ 写入 Iceberg（exactly-once）\n",
    "# ===============================\n",
    "blocks_query = (\n",
    "    blocks_df.writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/data/checkpoints/blocks_eth_mainnet\") \n",
    "    .toTable(\"iceberg.eth_blocks\")\n",
    ")\n",
    "\n",
    "state_query = (\n",
    "    state_df.writeStream\n",
    "    .format(\"iceberg\")\n",
    "    .outputMode(\"complete\")  # compact topic\n",
    "    .option(\"checkpointLocation\", \"/data/checkpoints/state_eth_mainnet\")\n",
    "    .toTable(\"iceberg.eth_ingestion_state\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 5️⃣ 启动流\n",
    "# ===============================\n",
    "blocks_query.awaitTermination()\n",
    "state_query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
