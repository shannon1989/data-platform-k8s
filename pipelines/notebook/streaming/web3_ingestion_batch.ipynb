{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "888567d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2026-01-23T08:13:22.812Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"quiknode\", \"key_envs\": [\"QUIKNODE_KEY\"], \"weight\": 8}\n",
      "{\"ts\": \"2026-01-23T08:13:22.813Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"infura\", \"key_envs\": [\"INFURA_KEY_A\", \"INFURA_KEY_B\", \"INFURA_KEY_C\"], \"weight\": 8}\n",
      "{\"ts\": \"2026-01-23T08:13:22.814Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"blockpi\", \"key_envs\": [\"BLOCKPI_BSC_KEY_A\", \"BLOCKPI_BSC_KEY_B\", \"BLOCKPI_BSC_KEY_C\"], \"weight\": 8}\n",
      "{\"ts\": \"2026-01-23T08:13:22.814Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"ankr\", \"key_envs\": [\"ANKR_KEY_A\", \"ANKR_KEY_B\", \"ANKR_KEY_C\"], \"weight\": 6}\n",
      "{\"ts\": \"2026-01-23T08:13:22.815Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"public-blxrbdn\", \"key_envs\": [\"public\"], \"weight\": 2}\n",
      "{\"ts\": \"2026-01-23T08:13:22.815Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"public-thirdweb\", \"key_envs\": [\"public\"], \"weight\": 1}\n",
      "{\"ts\": \"2026-01-23T08:13:22.816Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"public-nodies\", \"key_envs\": [\"public\"], \"weight\": 1}\n",
      "{\"ts\": \"2026-01-23T08:13:22.816Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"nownodes\", \"key_envs\": [\"NOWNODES_KEY_A\"], \"weight\": 5}\n",
      "{\"ts\": \"2026-01-23T08:13:22.817Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"rpc_enabled\", \"chain\": \"bsc\", \"rpc\": \"drpc\", \"key_envs\": [\"DRPC_API_KEY_A\", \"DRPC_API_KEY_B\"], \"weight\": 2}\n",
      "{\"ts\": \"2026-01-23T08:13:22.829Z\", \"level\": \"INFO\", \"logger\": \"web3_ingestion\", \"msg\": \"Kafka_initializing...\", \"bootstrap_servers\": \"redpanda.kafka.svc:9092\", \"transactional_id\": \"blockchain.ingestion.bsc.2026-01-23T08:13:22.812Z\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from confluent_kafka.serialization import SerializationContext, MessageField\n",
    "from prometheus_client import start_http_server\n",
    "from confluent_kafka import KafkaException\n",
    "from src.metrics import *\n",
    "from src.logging import log\n",
    "from src.rpc_provider import Web3AsyncRouter, AsyncRpcClient, AsyncRpcScheduler, RpcPool, RpcErrorResult, RpcTaskMeta\n",
    "from src.state import load_last_state\n",
    "from src.kafka_utils import init_producer, get_serializers, delivery_report\n",
    "from src.web3_utils import current_utctime, to_json_safe\n",
    "# from src.commit_timer import CommitTimer\n",
    "# from src.batch_executor import BatchContext, ParallelBatchExecutor\n",
    "from src.safe_latest import SafeLatestBlockProvider\n",
    "\n",
    "# -----------------------------\n",
    "# Environment Variables\n",
    "# -----------------------------\n",
    "RUN_ID = os.getenv(\"RUN_ID\", str(uuid.uuid4()))\n",
    "POLL_INTERVAL = float(os.getenv(\"POLL_INTERVAL\", \"1\")) # can be decimals\n",
    "CHAIN = os.getenv(\"CHAIN\", \"bsc\").lower() # bsc, eth, base ... from blockchain-rpc-config.yaml\n",
    "RESUME_FROM_LAST = os.getenv(\"RESUME_FROM_LAST\", \"True\").lower() in (\"1\", \"true\", \"yes\")\n",
    "\n",
    "# -----------------------------\n",
    "# Job Name & Kafka IDs\n",
    "# -----------------------------\n",
    "if RESUME_FROM_LAST:\n",
    "    JOB_NAME = f\"{CHAIN}_backfill_resume\"        # Âõ∫ÂÆöÂêçÔºåKafka checkpoint ËÉΩË¢´Â§çÁî®\n",
    "else:\n",
    "    JOB_NAME = f\"{CHAIN}_realtime_{current_utctime()}\"  # ÊØèÊ¨°ÂîØ‰∏ÄÔºå‰ªéÊúÄÊñ∞blockÂºÄÂßã\n",
    "\n",
    "# -----------------------------\n",
    "# RPC Config\n",
    "# -----------------------------\n",
    "RPC_CONFIG_PATH = \"/etc/ingestion/rpc_providers.json\"\n",
    "RPC_MAX_TIMEOUT = int(os.getenv(\"RPC_MAX_TIMEOUT\", \"10\"))\n",
    "RPC_MAX_SUBMIT = int(os.getenv(\"RPC_MAX_SUBMIT\", \"15\")) # how many blocks for every commit to Kafka\n",
    "RPC_MAX_INFLIGHT = int(os.getenv(\"RPC_MAX_INFLIGHT\", \"5\"))  # Âπ∂ÂèëÊï∞Èáè\n",
    "\n",
    "# -----------------------------\n",
    "# Log fetching Config\n",
    "# -----------------------------\n",
    "RANGE_SIZE = int(os.getenv(\"RANGE_SIZE\", \"5\")) # how many blocks of range to fetch for logs\n",
    "BATCH_TX_SIZE = int(os.getenv(\"BATCH_TX_SIZE\", \"5\"))  # Max 10 logs transaction per batch within a single block\n",
    "SLIDING_SIZE = int(os.getenv(\"SLIDING_SIZE\", \"20\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Kafka Config\n",
    "# -----------------------------\n",
    "TRANSACTIONAL_ID = f\"blockchain.ingestion.{CHAIN}.{current_utctime()}\" # TRANSACTIONAL_IDÊØèÊ¨°‰∏ç‰∏ÄÊ†∑ÔºåEOSÁî±Compact State TopicÂÆûÁé∞\n",
    "KAFKA_BROKER = \"redpanda.kafka.svc:9092\"\n",
    "SCHEMA_REGISTRY_URL = \"http://redpanda.kafka.svc:8081\"\n",
    "BLOCKS_TOPIC = f\"blockchain.logs.{CHAIN}\"\n",
    "STATE_TOPIC = f\"blockchain.state.{CHAIN}\"\n",
    "\n",
    "# -----------------------------\n",
    "# RPC Initilization\n",
    "# -----------------------------\n",
    "rpc_configs = json.load(open(RPC_CONFIG_PATH))\n",
    "rpc_pool = RpcPool.from_config(rpc_configs, CHAIN)\n",
    "\n",
    "# -----------------------------\n",
    "# Kafka Producer initialization\n",
    "# -----------------------------\n",
    "blocks_value_serializer, state_value_serializer = get_serializers(SCHEMA_REGISTRY_URL, BLOCKS_TOPIC, STATE_TOPIC)\n",
    "producer = init_producer(TRANSACTIONAL_ID, KAFKA_BROKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68b599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block Range ‰ªªÂä°\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BlockRangeTask:\n",
    "    range_id: int\n",
    "    start_block: int\n",
    "    end_block: int\n",
    "    \n",
    "# Range ÊâßË°åÁªìÊûúÔºàRPC ‚Üí Kafka ÁöÑÂçï‰ΩçÔºâ\n",
    "@dataclass\n",
    "class RangeResult:\n",
    "    range_id: int\n",
    "    start_block: int\n",
    "    end_block: int\n",
    "    logs: list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b3d098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range PlannerÔºàÈ°∫Â∫èÊòØ‰ªéËøôÈáåÂºÄÂßãÁöÑÔºâ\n",
    "def plan_block_ranges(start_block: int, end_block: int, range_size: int):\n",
    "    range_id = 0\n",
    "    b = start_block\n",
    "\n",
    "    while b <= end_block:\n",
    "        yield BlockRangeTask(\n",
    "            range_id=range_id,\n",
    "            start_block=b,\n",
    "            end_block=min(b + range_size - 1, end_block),\n",
    "        )\n",
    "        b += range_size\n",
    "        range_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92a5f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderedResultBufferÔºà‰øùËØÅÈ°∫Â∫èÊèê‰∫§Ôºâ- ingestion ÁöÑÁÅµÈ≠ÇÁªÑ‰ª∂\n",
    "class OrderedResultBuffer:\n",
    "    def __init__(self):\n",
    "        self._buffer = {}\n",
    "        self._next_range_id = 0\n",
    "\n",
    "    def add(self, result: RangeResult):\n",
    "        self._buffer[result.range_id] = result\n",
    "\n",
    "    def pop_ready(self):\n",
    "        ready = []\n",
    "        while self._next_range_id in self._buffer:\n",
    "            ready.append(self._buffer.pop(self._next_range_id))\n",
    "            self._next_range_id += 1\n",
    "        return ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0fc409e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fromBlock': '0 - hex(76776621)', 'toBlock': '0 - hex(76776621)'}]\n"
     ]
    }
   ],
   "source": [
    "last_block = 76776620\n",
    "start_block = last_block + 1\n",
    "end_block = start_block\n",
    "range_size = 1\n",
    "\n",
    "for task in plan_block_ranges(start_block, end_block, range_size):\n",
    "    params = [{\n",
    "        \"fromBlock\": f\"{task.range_id} - hex({task.start_block})\",\n",
    "        \"toBlock\": f\"{task.range_id} - hex({task.end_block})\",\n",
    "        # Âº∫ÁÉàÂª∫ËÆÆÔºöaddress / topics\n",
    "    }]\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857f09cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "producer = init_producer(TRANSACTIONAL_ID, KAFKA_BROKER)\n",
    "async def fetch_range_logs(\n",
    "    start_block: int,\n",
    "    end_block: int,\n",
    "    range_size: int,\n",
    "):\n",
    "    # -----------------------------\n",
    "    # RPC infra\n",
    "    # -----------------------------\n",
    "    client = AsyncRpcClient(timeout=RPC_MAX_TIMEOUT)\n",
    "    router = Web3AsyncRouter(rpc_pool, client)\n",
    "\n",
    "    scheduler = AsyncRpcScheduler(\n",
    "        router=router,\n",
    "        max_workers=1,                   # dispatcher Êï∞\n",
    "        max_inflight=RPC_MAX_INFLIGHT,    # ÁúüÊ≠£Âπ∂Âèë RPC\n",
    "        max_queue=RPC_MAX_INFLIGHT * 5,\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # Result ordering\n",
    "    # -----------------------------\n",
    "    ordered_buffer = OrderedResultBuffer()\n",
    "    \n",
    "    # -----------------------------\n",
    "    # 1Ô∏è‚É£ submit ÊâÄÊúâ block ranges\n",
    "    # -----------------------------\n",
    "    tasks = []\n",
    "\n",
    "    for task in plan_block_ranges(start_block, end_block, range_size):\n",
    "        params = [{\n",
    "            \"fromBlock\": hex(task.start_block),\n",
    "            \"toBlock\": hex(task.end_block),\n",
    "            # address / topics\n",
    "        }]\n",
    "\n",
    "        tasks.append(\n",
    "            asyncio.create_task(\n",
    "                scheduler.submit(\n",
    "                    \"eth_getLogs\",\n",
    "                    params,\n",
    "                    meta={\n",
    "                        \"range_id\": task.range_id,\n",
    "                        \"start_block\": task.start_block,\n",
    "                        \"end_block\": task.end_block,\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # ‚ö†Ô∏è gather ‰øùËØÅËøîÂõûÈ°∫Â∫è = submit È°∫Â∫è\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2Ô∏è‚É£ Â§ÑÁêÜ RPC ÁªìÊûúÔºà‰π±Â∫è ‚Üí ÊúâÂ∫èÔºâ\n",
    "    # -----------------------------\n",
    "    for r in results:\n",
    "        if isinstance(r, RpcErrorResult):\n",
    "            log.error(\n",
    "                \"rpc_range_failed\",\n",
    "                extra={\n",
    "                    \"range_id\": r.meta.extra.get(\"range_id\"),\n",
    "                    \"provider\": r.rpc,\n",
    "                    \"key\": r.key_env,\n",
    "                    \"error\": type(r.error).__name__,\n",
    "                },\n",
    "            )\n",
    "            raise RuntimeError(\"range rpc failed\")  # Áîü‰∫ßÂèØÊç¢Êàê retry/split\n",
    "\n",
    "        logs, rpc, key_env, trace, wid, meta = r\n",
    "\n",
    "        range_result = RangeResult(\n",
    "            range_id=meta.extra[\"range_id\"],\n",
    "            start_block=meta.extra[\"start_block\"],\n",
    "            end_block=meta.extra[\"end_block\"],\n",
    "            logs=logs or [],   # üî• Á©∫ÂùóÊòØÊàêÂäü\n",
    "        )\n",
    "\n",
    "        ordered_buffer.add(range_result)\n",
    "\n",
    "        # -----------------------------\n",
    "        # 3Ô∏è‚É£ Â∞ùËØïÈ°∫Â∫èÂÜô Kafka\n",
    "        # -----------------------------\n",
    "        ready_ranges = ordered_buffer.pop_ready()\n",
    "\n",
    "        for rr in ready_ranges:\n",
    "            producer.begin_transaction()\n",
    "            \n",
    "            batch_tx_total = 0\n",
    "            block_count = 0\n",
    "\n",
    "            range_logs = rr.logs\n",
    "            \n",
    "            if range_logs is None:\n",
    "                raise RuntimeError(\n",
    "                    f\"range logs {start_block}-{end_block} fetch failed\"\n",
    "                )\n",
    "\n",
    "            range_logs_safe = to_json_safe(range_logs)\n",
    "\n",
    "            if not isinstance(range_logs_safe, list):\n",
    "                raise RuntimeError(\n",
    "                    f\"Unexpected range_logs type: {type(range_logs_safe)}\"\n",
    "                )\n",
    "\n",
    "            # -----------------------------------\n",
    "            # Êåâ blockNumber ÂàÜÁªÑ\n",
    "            # -----------------------------------\n",
    "            logs_by_block = {}\n",
    "            \n",
    "            for log_item in range_logs_safe:\n",
    "                bn = log_item.get(\"blockNumber\")\n",
    "                if bn is None:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(bn, str):\n",
    "                    bn = int(bn, 16)\n",
    "\n",
    "                logs_by_block.setdefault(bn, []).append(log_item)\n",
    "\n",
    "            # -----------------------------------\n",
    "            # ÈÄê block Â§ÑÁêÜ\n",
    "            # -----------------------------------\n",
    "            for bn, transactions in logs_by_block.items():\n",
    "                total_tx = len(transactions)\n",
    "                batch_tx_total += total_tx\n",
    "                block_count += 1\n",
    "\n",
    "                for start_idx in range(0, total_tx, BATCH_TX_SIZE):\n",
    "                    batch_tx = transactions[\n",
    "                        start_idx : start_idx + BATCH_TX_SIZE\n",
    "                    ]\n",
    "\n",
    "                    for idx, tx in enumerate(batch_tx, start=start_idx):\n",
    "                        tx_record = {\n",
    "                            \"block_height\": bn,\n",
    "                            \"job_name\": JOB_NAME,\n",
    "                            \"run_id\": RUN_ID,\n",
    "                            \"inserted_at\": current_utctime(),\n",
    "                            \"raw\": json.dumps(tx),\n",
    "                            \"tx_index\": idx,\n",
    "                        }\n",
    "\n",
    "                        producer.produce(\n",
    "                            topic=BLOCKS_TOPIC,\n",
    "                            key=f\"{bn}-{idx}\",\n",
    "                            value=blocks_value_serializer(\n",
    "                                tx_record,\n",
    "                                SerializationContext(\n",
    "                                    BLOCKS_TOPIC, MessageField.VALUE\n",
    "                                ),\n",
    "                            ),\n",
    "                            on_delivery=delivery_report,\n",
    "                        )\n",
    "\n",
    "                    producer.poll(0)\n",
    "            \n",
    "            # -----------------------------\n",
    "            # Commit state\n",
    "            # -----------------------------\n",
    "            state_record = {\n",
    "                \"job_name\": JOB_NAME,\n",
    "                \"run_id\": RUN_ID,\n",
    "                \"range\": {\"start\": meta.extra[\"start_block\"], \"end\": meta.extra[\"end_block\"]},\n",
    "                \"checkpoint\": meta.extra[\"end_block\"],\n",
    "                \"status\": \"running\",\n",
    "                \"inserted_at\": current_utctime(),\n",
    "            }\n",
    "            # for log_item in rr.logs:\n",
    "            #     producer.send(\n",
    "            #         topic=LOG_TOPIC,\n",
    "            #         key=str(rr.range_id).encode(),\n",
    "            #         value=serialize_log(log_item),\n",
    "            #     )\n",
    "\n",
    "            # üî• range ÊàêÂäü ‚Üí Êé®Ëøõ checkpoint\n",
    "            # commit_coordinator.commit(rr.end_block)\n",
    "\n",
    "            producer.produce(\n",
    "                STATE_TOPIC,\n",
    "                key=JOB_NAME,\n",
    "                value=state_value_serializer(\n",
    "                    state_record,\n",
    "                    SerializationContext(STATE_TOPIC, MessageField.VALUE),\n",
    "                ),\n",
    "                on_delivery=delivery_report,\n",
    "            )\n",
    "            producer.poll(0)\n",
    "            \n",
    "            producer.commit_transaction()\n",
    "\n",
    "            log.info(\n",
    "                \"range_committed\",\n",
    "                extra={\n",
    "                    \"range_id\": meta.extra[\"range_id\"],\n",
    "                    \"start\": meta.extra[\"start_block\"],\n",
    "                    \"end\": meta.extra[\"end_block\"],\n",
    "                    \"rpc\": rpc,\n",
    "                    \"key_env\": key_env,\n",
    "                    \"log_count\": len(rr.logs),\n",
    "                },\n",
    "            )\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4Ô∏è‚É£ graceful shutdown\n",
    "    # -----------------------------\n",
    "    await scheduler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d270ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_block = 76776621\n",
    "start_block = last_block + 1\n",
    "end_block = start_block + RPC_MAX_INFLIGHT * 5 - 1\n",
    "range_size = 5\n",
    "\n",
    "await fetch_range_logs(start_block, end_block, range_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
